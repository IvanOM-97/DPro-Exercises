{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1LReEgrWgkf+HM8TFFyU8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvanOM-97/DPro-Exercises/blob/master/U44T2C96TensorflowSeriesAssignments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuaxL1dQzb9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1120cbc3-40d6-46be-abf5-559e2e814aad",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: TensorFlow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from TensorFlow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->TensorFlow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->TensorFlow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->TensorFlow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->TensorFlow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->TensorFlow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->TensorFlow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->TensorFlow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->TensorFlow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->TensorFlow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->TensorFlow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->TensorFlow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->TensorFlow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->TensorFlow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->TensorFlow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->TensorFlow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "'''\n",
        "TENSORFLOW\n",
        "\n",
        "'''\n",
        "!pip install TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROBLEMA 1 - MIRANDO HACIA ATRAS DESDE SCRATCH\n",
        "  Looking back at the scratches so far, when using scratch implementation of neural networks had to manually implement:\n",
        "\n",
        "  1. Weight initialization\n",
        "  2. I had to define forward propagation\n",
        "  3. I needed to calculate loss, including mean squared error and cross-entropy.\n",
        "  4. I implemented Backpropagation\n",
        "  5. Implemented gradient parameter updates\n",
        "  6. I needed an epoch loop\n",
        "  7. I implemented Mini-batch processing\n",
        "  8. I also did accuracy evaluation\n",
        "  9. I also performed preprocessing of data data load, split, and normalize.\n",
        "'''"
      ],
      "metadata": {
        "id": "KHUnHWTwLhqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "PROBLEMA 2 - CONSIDERANDO LA COPATIBILIDAD ENTRE SCRATCH Y TENSORFLOW\n",
        "  This is how TensorFlow implements the following:\n",
        "\n",
        "  1. Weight initialization - uses tf.Variable(tf.random_normal(...))\n",
        "  2. Forward propagation - done via tf.matmul, tf.add, tf.nn.relu\n",
        "  3. Loss calculation - achieved by tf.nn.sigmoid_cross_entropy_with_logits\n",
        "  4. Backpropagation - one by optimizer.minimize(loss_op)\n",
        "  5. Parameter update(gradient) - achieved by train_op = optimizer.minimize(...)\n",
        "  6. Epoch loop - done via for epoch in range(num_epochs)\n",
        "  7. Mini-batch handling - is GetMiniBatch class with for x, y in ...\n",
        "  8. Accuracy calculation - implemented via tf.equal(...), tf.reduce_mean(...)\n",
        "  9. Data preprocessing - achieved by pandas, numpy, and sklearn\n",
        "'''\n",
        "\n",
        "# Sample Code\n",
        "\"\"\"\n",
        "Using a neural network implemented in TensorFlow to perform binary classification on the Iris dataset\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "#import tensorflow as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "tf.test.gpu_device_name()\n",
        "\"\"\"\n",
        "When changing the TensorFlow version to the 1.x series, don't forget to install the GPU version with \"!pip install tensorflow-gpu==1.14.0\".\n",
        "Use tf.test.gpu_device_name() to check the GPU configuration status and verify if it is recognized.\n",
        "If successful, logs will be output; if not recognized, nothing will be output.\n",
        "\"\"\"\n",
        "\n",
        "# Loading the dataset\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "\n",
        "# Conditional extraction from DataFrame\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\") | (df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "\n",
        "# Convert to NumPy array\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "# Convert labels to numerical values\n",
        "y[y == \"Iris-versicolor\"] = 0\n",
        "y[y == \"Iris-virginica\"] = 1\n",
        "y = y.astype(np.int64)[:, np.newaxis]\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# Further split train into train and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    Iterator to get mini-batches\n",
        "      Parameters\n",
        "      ----------\n",
        "      X : ndarray of shape (n_samples, n_features)\n",
        "        Training data\n",
        "      y : ndarray of shape (n_samples, 1)\n",
        "        Ground truth values\n",
        "      batch_size : int\n",
        "        Batch size\n",
        "      seed : int\n",
        "        NumPy random seed\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "\n",
        "# Hyperparameter configuration\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# Define the shape of arguments to pass to the computational graph\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# Mini-batch iterator for training\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"\n",
        "    単純な3層ニューラルネットワーク\n",
        "    \"\"\"\n",
        "    tf.random.set_random_seed(0)\n",
        "    # Declaration of weights and biases\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n",
        "    return layer_output\n",
        "\n",
        "# Loading the network structure\n",
        "logits = example_net(X)\n",
        "\n",
        "# Objective function\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# Optimization method\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# Prediction results\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "# Metric calculation\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Variable initialization\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "\n",
        "# Execution of the computational graph\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        # Loop for each epoch\n",
        "        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int64)\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # Loop for each mini-batch\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "            total_loss += loss\n",
        "        total_loss /= n_samples\n",
        "        val_loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}\".format(epoch, total_loss, val_loss, acc))\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyivwJEyObhG",
        "outputId": "28e7fc0d-1b5f-4cf5-c4b6-905f834883d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 7.0241, val_loss : 67.6860, acc : 0.375\n",
            "Epoch 1, loss : 3.4241, val_loss : 23.4026, acc : 0.312\n",
            "Epoch 2, loss : 1.9387, val_loss : 11.6681, acc : 0.375\n",
            "Epoch 3, loss : 2.0917, val_loss : 13.1400, acc : 0.312\n",
            "Epoch 4, loss : 1.7685, val_loss : 17.7284, acc : 0.312\n",
            "Epoch 5, loss : 1.6097, val_loss : 12.9607, acc : 0.312\n",
            "Epoch 6, loss : 1.4402, val_loss : 10.0593, acc : 0.312\n",
            "Epoch 7, loss : 1.3704, val_loss : 9.4797, acc : 0.312\n",
            "Epoch 8, loss : 1.2536, val_loss : 9.8518, acc : 0.312\n",
            "Epoch 9, loss : 1.1476, val_loss : 8.5670, acc : 0.375\n",
            "Epoch 10, loss : 1.0930, val_loss : 8.0430, acc : 0.375\n",
            "Epoch 11, loss : 1.0412, val_loss : 7.8791, acc : 0.375\n",
            "Epoch 12, loss : 0.9804, val_loss : 7.1233, acc : 0.375\n",
            "Epoch 13, loss : 0.9326, val_loss : 6.7908, acc : 0.375\n",
            "Epoch 14, loss : 0.8792, val_loss : 6.2492, acc : 0.375\n",
            "Epoch 15, loss : 0.8304, val_loss : 5.7680, acc : 0.375\n",
            "Epoch 16, loss : 0.7835, val_loss : 5.2886, acc : 0.438\n",
            "Epoch 17, loss : 0.7384, val_loss : 4.8037, acc : 0.438\n",
            "Epoch 18, loss : 0.6961, val_loss : 4.3575, acc : 0.500\n",
            "Epoch 19, loss : 0.6543, val_loss : 3.9175, acc : 0.500\n",
            "Epoch 20, loss : 0.6136, val_loss : 3.5188, acc : 0.500\n",
            "Epoch 21, loss : 0.5738, val_loss : 3.1371, acc : 0.500\n",
            "Epoch 22, loss : 0.5349, val_loss : 2.7697, acc : 0.500\n",
            "Epoch 23, loss : 0.4973, val_loss : 2.4528, acc : 0.562\n",
            "Epoch 24, loss : 0.4606, val_loss : 2.1728, acc : 0.562\n",
            "Epoch 25, loss : 0.4255, val_loss : 1.9324, acc : 0.625\n",
            "Epoch 26, loss : 0.3919, val_loss : 1.7049, acc : 0.625\n",
            "Epoch 27, loss : 0.3609, val_loss : 1.5248, acc : 0.688\n",
            "Epoch 28, loss : 0.3326, val_loss : 1.3724, acc : 0.750\n",
            "Epoch 29, loss : 0.3071, val_loss : 1.2386, acc : 0.750\n",
            "Epoch 30, loss : 0.2851, val_loss : 1.1454, acc : 0.750\n",
            "Epoch 31, loss : 0.2647, val_loss : 1.0364, acc : 0.750\n",
            "Epoch 32, loss : 0.2466, val_loss : 0.9368, acc : 0.750\n",
            "Epoch 33, loss : 0.2297, val_loss : 0.8304, acc : 0.750\n",
            "Epoch 34, loss : 0.2155, val_loss : 0.7243, acc : 0.750\n",
            "Epoch 35, loss : 0.2024, val_loss : 0.6215, acc : 0.750\n",
            "Epoch 36, loss : 0.1920, val_loss : 0.5405, acc : 0.812\n",
            "Epoch 37, loss : 0.1815, val_loss : 0.4515, acc : 0.812\n",
            "Epoch 38, loss : 0.1735, val_loss : 0.4016, acc : 0.812\n",
            "Epoch 39, loss : 0.1643, val_loss : 0.3323, acc : 0.812\n",
            "Epoch 40, loss : 0.1574, val_loss : 0.2935, acc : 0.875\n",
            "Epoch 41, loss : 0.1496, val_loss : 0.2519, acc : 0.875\n",
            "Epoch 42, loss : 0.1429, val_loss : 0.2206, acc : 0.875\n",
            "Epoch 43, loss : 0.1364, val_loss : 0.1932, acc : 0.875\n",
            "Epoch 44, loss : 0.1302, val_loss : 0.1684, acc : 0.875\n",
            "Epoch 45, loss : 0.1244, val_loss : 0.1464, acc : 0.875\n",
            "Epoch 46, loss : 0.1188, val_loss : 0.1264, acc : 0.875\n",
            "Epoch 47, loss : 0.1139, val_loss : 0.1097, acc : 0.875\n",
            "Epoch 48, loss : 0.1091, val_loss : 0.0947, acc : 0.938\n",
            "Epoch 49, loss : 0.1050, val_loss : 0.0833, acc : 0.938\n",
            "Epoch 50, loss : 0.1007, val_loss : 0.0712, acc : 1.000\n",
            "Epoch 51, loss : 0.0975, val_loss : 0.0653, acc : 1.000\n",
            "Epoch 52, loss : 0.0932, val_loss : 0.0531, acc : 1.000\n",
            "Epoch 53, loss : 0.0915, val_loss : 0.0557, acc : 1.000\n",
            "Epoch 54, loss : 0.0862, val_loss : 0.0390, acc : 1.000\n",
            "Epoch 55, loss : 0.0875, val_loss : 0.0598, acc : 0.938\n",
            "Epoch 56, loss : 0.0790, val_loss : 0.0376, acc : 1.000\n",
            "Epoch 57, loss : 0.0871, val_loss : 0.0935, acc : 0.938\n",
            "Epoch 58, loss : 0.0723, val_loss : 0.0861, acc : 0.938\n",
            "Epoch 59, loss : 0.0936, val_loss : 0.1730, acc : 0.938\n",
            "Epoch 60, loss : 0.0693, val_loss : 0.1648, acc : 0.938\n",
            "Epoch 61, loss : 0.1047, val_loss : 0.2713, acc : 0.938\n",
            "Epoch 62, loss : 0.0711, val_loss : 0.1687, acc : 0.938\n",
            "Epoch 63, loss : 0.1072, val_loss : 0.3121, acc : 0.938\n",
            "Epoch 64, loss : 0.0738, val_loss : 0.1538, acc : 0.938\n",
            "Epoch 65, loss : 0.1069, val_loss : 0.3194, acc : 0.938\n",
            "Epoch 66, loss : 0.0757, val_loss : 0.1525, acc : 0.938\n",
            "Epoch 67, loss : 0.1082, val_loss : 0.3200, acc : 0.938\n",
            "Epoch 68, loss : 0.0780, val_loss : 0.1584, acc : 0.938\n",
            "Epoch 69, loss : 0.1114, val_loss : 0.3153, acc : 0.938\n",
            "Epoch 70, loss : 0.0808, val_loss : 0.1600, acc : 0.938\n",
            "Epoch 71, loss : 0.1139, val_loss : 0.2888, acc : 0.938\n",
            "Epoch 72, loss : 0.0824, val_loss : 0.1424, acc : 0.938\n",
            "Epoch 73, loss : 0.1110, val_loss : 0.2301, acc : 0.938\n",
            "Epoch 74, loss : 0.0791, val_loss : 0.1062, acc : 0.938\n",
            "Epoch 75, loss : 0.1002, val_loss : 0.1769, acc : 0.938\n",
            "Epoch 76, loss : 0.0719, val_loss : 0.0579, acc : 0.938\n",
            "Epoch 77, loss : 0.0810, val_loss : 0.0948, acc : 0.938\n",
            "Epoch 78, loss : 0.0610, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 79, loss : 0.0601, val_loss : 0.0222, acc : 1.000\n",
            "Epoch 80, loss : 0.0514, val_loss : 0.0117, acc : 1.000\n",
            "Epoch 81, loss : 0.0498, val_loss : 0.0060, acc : 1.000\n",
            "Epoch 82, loss : 0.0454, val_loss : 0.0173, acc : 1.000\n",
            "Epoch 83, loss : 0.0472, val_loss : 0.0077, acc : 1.000\n",
            "Epoch 84, loss : 0.0430, val_loss : 0.0289, acc : 1.000\n",
            "Epoch 85, loss : 0.0449, val_loss : 0.0094, acc : 1.000\n",
            "Epoch 86, loss : 0.0400, val_loss : 0.0493, acc : 0.938\n",
            "Epoch 87, loss : 0.0430, val_loss : 0.0089, acc : 1.000\n",
            "Epoch 88, loss : 0.0374, val_loss : 0.0669, acc : 0.938\n",
            "Epoch 89, loss : 0.0429, val_loss : 0.0068, acc : 1.000\n",
            "Epoch 90, loss : 0.0361, val_loss : 0.0709, acc : 0.938\n",
            "Epoch 91, loss : 0.0422, val_loss : 0.0058, acc : 1.000\n",
            "Epoch 92, loss : 0.0349, val_loss : 0.0692, acc : 0.938\n",
            "Epoch 93, loss : 0.0411, val_loss : 0.0055, acc : 1.000\n",
            "Epoch 94, loss : 0.0337, val_loss : 0.0678, acc : 0.938\n",
            "Epoch 95, loss : 0.0397, val_loss : 0.0059, acc : 1.000\n",
            "Epoch 96, loss : 0.0326, val_loss : 0.0613, acc : 0.938\n",
            "Epoch 97, loss : 0.0376, val_loss : 0.0081, acc : 1.000\n",
            "Epoch 98, loss : 0.0316, val_loss : 0.0499, acc : 0.938\n",
            "Epoch 99, loss : 0.0349, val_loss : 0.0138, acc : 1.000\n",
            "test_acc : 0.900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROBLEMA 3 - CREA A MODELO PARA IRIS UTILIZANDO LAS 3 VARIABLES TARGET (CLASIFICACION)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Loading Iris Dataset\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "df = pd.read_csv(\"Iris.csv\")\n",
        "X = df[[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]].values\n",
        "y = df[\"Species\"].values\n",
        "\n",
        "# One-hot encode labels\n",
        "encoder = LabelBinarizer()\n",
        "y = encoder.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_hidden1, n_hidden2 = 50, 100\n",
        "n_classes = 3\n",
        "n_samples = X_train.shape[0]\n",
        "\n",
        "# Placeholders\n",
        "X_ph = tf.placeholder(tf.float32, [None, n_input])\n",
        "Y_ph = tf.placeholder(tf.float32, [None, n_classes])\n",
        "\n",
        "# Weights and biases\n",
        "weights = {\n",
        "    'w1' : tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "    'w2' : tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "    'w3' : tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "}\n",
        "biases = {\n",
        "    'b1' : tf.Variable(tf.random_normal([n_hidden1])),\n",
        "    'b2' : tf.Variable(tf.random_normal([n_hidden2])),\n",
        "    'b3' : tf.Variable(tf.random_normal([n_classes]))\n",
        "}\n",
        "\n",
        "# Model\n",
        "def model(x):\n",
        "    l1 = tf.nn.relu(tf.add(tf.matmul(x, weights['w1']), biases['b1']))\n",
        "    l2 = tf.nn.relu(tf.add(tf.matmul(l1, weights['w2']), biases['b2']))\n",
        "    return tf.add(tf.matmul(l2, weights['w3']), biases['b3'])\n",
        "\n",
        "logits = model(X_ph)\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y_ph, logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op)\n",
        "\n",
        "# Accuracy\n",
        "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y_ph, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Training\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(num_epochs):\n",
        "        for i in range(0, n_samples, batch_size):\n",
        "            x_batch = X_train[i:i+batch_size]\n",
        "            y_batch = y_train[i:i+batch_size]\n",
        "            sess.run(optimizer, feed_dict={X_ph: x_batch, Y_ph: y_batch})\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X_ph: X_val, Y_ph: y_val})\n",
        "            print(f\"Epoch {epoch}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    test_acc = sess.run(accuracy, feed_dict={X_ph: X_test, Y_ph: y_test})\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU_lwtTOVqts",
        "outputId": "bf4f796c-2f0f-4c65-e7eb-980e8b8aedee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/util/dispatch.py:1260: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Validation Loss: 98.3034, Validation Accuracy: 0.4583\n",
            "Epoch 10, Validation Loss: 0.6594, Validation Accuracy: 0.8333\n",
            "Epoch 20, Validation Loss: 0.6042, Validation Accuracy: 0.9167\n",
            "Epoch 30, Validation Loss: 0.5235, Validation Accuracy: 0.8750\n",
            "Epoch 40, Validation Loss: 0.4777, Validation Accuracy: 0.9583\n",
            "Epoch 50, Validation Loss: 0.4499, Validation Accuracy: 0.9167\n",
            "Epoch 60, Validation Loss: 0.4432, Validation Accuracy: 0.9167\n",
            "Epoch 70, Validation Loss: 0.4385, Validation Accuracy: 0.8750\n",
            "Epoch 80, Validation Loss: 0.4333, Validation Accuracy: 0.8750\n",
            "Epoch 90, Validation Loss: 0.4284, Validation Accuracy: 0.8750\n",
            "Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROBLEMA 4 - CREA UN MODELO PARA HOUSE PRICES (REGRESION)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow.compat.v1 as tf\n",
        "#import tensorflow as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Loading the dataset\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "df = df[[\"GrLivArea\", \"YearBuilt\", \"SalePrice\"]].dropna()\n",
        "X = df[[\"GrLivArea\", \"YearBuilt\"]].values\n",
        "y = df[\"SalePrice\"].values.reshape(-1, 1)\n",
        "\n",
        "# Normalizing\n",
        "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "y = (y - y.mean()) / y.std()\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "# Hyperparameter configuration\n",
        "batch_size = 10\n",
        "\n",
        "# Placeholder\n",
        "X_ph = tf.placeholder(tf.float32, [None, 2])\n",
        "y_ph = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "# Weigghts and biases\n",
        "W1 = tf.Variable(tf.random_normal([2, 64]))\n",
        "b1 = tf.Variable(tf.zeros([64]))\n",
        "W2 = tf.Variable(tf.random_normal([64, 1]))\n",
        "b2 = tf.Variable(tf.zeros([1]))\n",
        "\n",
        "# Model\n",
        "l1 = tf.nn.relu(tf.matmul(X_ph, W1) + b1)\n",
        "output = tf.matmul(l1, W2) + b2\n",
        "\n",
        "# Loss and optimizer\n",
        "loss = tf.reduce_mean(tf.square(output - y_ph))\n",
        "optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n",
        "\n",
        "# Training\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(100):\n",
        "        for i in range(0, len(X_train), batch_size):\n",
        "            x_batch = X_train[i:i+10]\n",
        "            y_batch = y_train[i:i+10]\n",
        "            sess.run(optimizer, feed_dict={X_ph: x_batch, y_ph: y_batch})\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            val_loss = sess.run(loss, feed_dict={X_ph: X_val, y_ph: y_val})\n",
        "            print(f\"Epoch {epoch}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    test_loss = sess.run(loss, feed_dict={X_ph: X_test, y_ph: y_test})\n",
        "    print(f\"Test Loss: {test_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi1jB_dHO0If",
        "outputId": "b9caf332-a4d9-40b4-b230-81ad8eac2d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Validation Loss: 0.4406\n",
            "Epoch 10, Validation Loss: 0.3585\n",
            "Epoch 20, Validation Loss: 0.3512\n",
            "Epoch 30, Validation Loss: 0.3416\n",
            "Epoch 40, Validation Loss: 0.3471\n",
            "Epoch 50, Validation Loss: 0.3272\n",
            "Epoch 60, Validation Loss: 0.3051\n",
            "Epoch 70, Validation Loss: 0.2885\n",
            "Epoch 80, Validation Loss: 0.2828\n",
            "Epoch 90, Validation Loss: 0.2850\n",
            "Test Loss: 0.5117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PROBLEMA 5 - CREA UN MODELO MNIST (CLASIFICACION DE IMAGENES)\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf. disable_v2_behavior()\n",
        "\n",
        "# Loading MNIST from tf.keras\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalizing and reshaping\n",
        "x_train = X_train.reshape(-1, 784) / 255.0\n",
        "x_test = X_test.reshape(-1, 784) / 255.0\n",
        "\n",
        "# One-hot encode labels\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(sparse_output=False)\n",
        "y_train = enc.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test = enc.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Splitting validation set\n",
        "x_val = x_train[-5000:]\n",
        "y_val = y_train[-5000:]\n",
        "x_train = x_train[:-5000]\n",
        "y_train = y_train[:-5000]\n",
        "\n",
        "# Placeholders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# Model parameters\n",
        "W1 = tf.Variable(tf.random_normal([784, 128]))\n",
        "b1 = tf.Variable(tf.zeros([128]))\n",
        "W2 = tf.Variable(tf.random_normal([128, 10]))\n",
        "b2 = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# Building model\n",
        "l1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "logits = tf.matmul(l1, W2) + b2\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss_op)\n",
        "\n",
        "# Accuracy\n",
        "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# Training\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    for epoch in range(10):\n",
        "        for i in range(0, len(x_train), 100):\n",
        "            x_batch = x_train[i:i+100]\n",
        "            y_batch = y_train[i:i+100]\n",
        "            sess.run(optimizer, feed_dict={X: x_batch, Y: y_batch})\n",
        "\n",
        "        val_acc = sess.run(accuracy, feed_dict={X: x_val, Y: y_val})\n",
        "        print(f\"Epoch {epoch}, Validation Accuracy: {val_acc:.4f}\")\n",
        "\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: x_test, Y: y_test})\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTmMlrG8Yk-N",
        "outputId": "1cf88a35-76e4-49e7-a650-cebe8d317753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.12/dist-packages/tensorflow/python/util/dispatch.py:1260: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Validation Accuracy: 0.8202\n",
            "Epoch 1, Validation Accuracy: 0.8754\n",
            "Epoch 2, Validation Accuracy: 0.8926\n",
            "Epoch 3, Validation Accuracy: 0.9036\n",
            "Epoch 4, Validation Accuracy: 0.9112\n",
            "Epoch 5, Validation Accuracy: 0.9182\n",
            "Epoch 6, Validation Accuracy: 0.9226\n",
            "Epoch 7, Validation Accuracy: 0.9276\n",
            "Epoch 8, Validation Accuracy: 0.9318\n",
            "Epoch 9, Validation Accuracy: 0.9342\n",
            "Test Accuracy: 0.9230\n"
          ]
        }
      ]
    }
  ]
}